{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 号称最好的中文分词Python模块\n",
    "P.S. : 以下的比较内容参考自jieba库作者的github的主页使用教程  \n",
    "## 算法\n",
    "* 前缀树\n",
    "* 动态规划\n",
    "* 采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法\n",
    "## 功能\n",
    "1.分词  \n",
    "    支持三种分词模式：\n",
    "        * 精确模式，试图将句子最精确地切开，适合文本分析；\n",
    "        * 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；\n",
    "        * 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。\n",
    "    带分词的字符串可以是 UTF-8 , Unicode(意味着python3内置的编码模式可以直接使用) , GBK(极度不推荐)\n",
    "    1. 函数\n",
    "        * jieba.cut(string , cut_all , HMM) : string是要分词的字符串，cut_all是选择是否采用全模式，HMM决定是否采用HMM模型,返回迭代器\n",
    "        * jieba.cut_for_search（string , HMM） : string是要分词的字符串，HMM参数决定是否采用HMM模型，分词的粒度比较细，返回迭代器\n",
    "        * jieba.lcut : 同上返回列表\n",
    "        * jieba.cut_for_search : 同上返回列表\n",
    "        * jieba.Tokenizer(dictionary=DEFAULT_DICT) : 新建自定义分词器，可用于同时使用不同词典。\n",
    "        * jieba.dt : jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n",
      "他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.自定义词典  \n",
    "* 建议为jieba载入jieba没有识别的词语，可以用户载入自定义字典\n",
    "* 函数 :\n",
    "    * jieba.load_userdict(file_name) : 载入用户自定义的字典文件，词典是是有格式的  \n",
    "      词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。**词频省略的时候可以自动计算**\n",
    "    * add_word()\n",
    "    * del_word()\n",
    "    * suggest_freq(segment, tune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/post/中将/出错/。\n",
      "如果/放到/post/中/将/出错/。\n",
      "「/台/中/」/正确/应该/不会/被/切开\n",
      "「/台中/」/正确/应该/不会/被/切开\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))\n",
    "jieba.suggest_freq(('中', '将'), True)\n",
    "print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))\n",
    "print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))\n",
    "jieba.suggest_freq('台中', True)\n",
    "print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.关键词提取  \n",
    "* TF-IDF 算法\n",
    "import jieba.analyse\n",
    "\n",
    "    * jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n",
    "        * sentence 为待提取的文本\n",
    "        * topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n",
    "        * withWeight 为是否一并返回关键词权重值，默认值为 False\n",
    "        * allowPOS 仅包括指定词性的词，默认值为空，即不筛选\n",
    "    * jieba.analyse.TFIDF(idf_path=None) 新建 TFIDF 实例，idf_path 为 IDF 频率文件\n",
    "* TextRank 算法\n",
    "\n",
    "    * jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。\n",
    "    * jieba.analyse.TextRank() 新建自定义 TextRank 实例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['舍友', '直播', '正在']\n",
      "['舍友', '直播']\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as ja\n",
    "print(ja.extract_tags('舍友正在看直播'))\n",
    "print(ja.textrank(\"舍友正在看直播\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.词性标注  \n",
    "   * n : 名词\n",
    "   * v : 动词\n",
    "   * a : 形容词\n",
    "   * b : 区别词\n",
    "   * z : 状态词\n",
    "   * r : 代词\n",
    "   * m : 数词\n",
    "   * q : 量词\n",
    "   * d : 副词\n",
    "   * p : 介词\n",
    "   * c : 连词\n",
    "   * u : 助词\n",
    "   * e : 叹词\n",
    "   * y : 语气词\n",
    "   * o : 拟声词\n",
    "   * h : 前缀\n",
    "   * k : 后缀\n",
    "   * w : 标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "北京 ns\n",
      "天安门 ns\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我爱北京天安门\")\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
