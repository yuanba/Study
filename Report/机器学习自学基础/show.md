## Logistic

解决问题

1. 两分类
2. 线性可回归问题

复习一下回归的定义 : 加入现在又一些数据点，我们用一条直线对这些点做拟合，期望得到最佳拟合直线，拟合的过程叫做回归

在拟合问题中，主要的思想在于我们拟合良好的数据预测回归公式

Logistic就是经典的一个拟合的实例，这是一个标准的二值型输出分类器 : 

我们对于每一个数据集的特征都创建一个对应的回归系数，每一个特征对最后的分类的结果都存在贡献，我们目的就是对于拟合一组这样的系数使得我们对于分类的结果在训练集上很优秀，并在之后的数据集上泛化的很好

$$z=w_0x_0+w_1x_1+w_2x_2+...+w_nx_n$$

在这里我们的$$X$$是分类器的输入数据，$$W$$是迭代找的最佳系数

### Gradient descent

高数中我们都学习如何快速的求一个函数的最优值，我们通常使用的一个重要的工具叫做梯度

我们可以理解梯度就是我们朝最优值变化的一个趋势，沿这个趋势可以最快的到达目的地(当然这是微分上的概念，实际上我们还需要考虑**步长(震荡)**和局部最优的情况)

$$x_{k+1} = x_k + \rho_k\bigtriangledown^{k}$$

这里的公式就渗透了我们的利用梯度求解最优值的迭代过程

问题

1. 但是这里会引出来一个之后会显现的很重要的问题 : 步长(步长太短算法收敛的很慢，步长太大，算法容易发散并且会出现频繁的在最有值附近震荡的情况)，解决办法主要办法就是动态步长选取,这是后话了，之后会讲到
2. 迭代次数(迭代多少次可以处于良好的误差范围中)

#### BGD

批量梯度下降算法，是最原始的一种梯度下降算法，对于整个数据集进行遍历操作，可以尽可能的保证我们的回归系数的精度，但是带来的缺点显而易见

一旦我们的数据集的个数和我们的特征的数目变得异常的巨大，我们的计算量是非常的庞大的，甚至是不可接收的

#### SGD

为了弥补BGD的缺陷，SGD的核心在于在数据量庞大的时候，我们随机抽取的数据集其实可以当做整体数据集影响的近似估计，我们从训练集中抽取出一小批量的样本作为本次迭代的数据集(该样本集的大小基本不随数据量的变大而变化)

### Logistic or Sigmod

在logistic 和 Sigmod都讲完之后演示两张图表示出回归的效果的差距(同迭代次数下)

### ML

## K-Means

### Hyperparameter and validation set

### KFoldXV

## Difficulty

### Curse of dimensionality

### PCA

### Maniford

## Create some ML Algorithm 

